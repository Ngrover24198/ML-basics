{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "z_VnTK4Up65x"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input,LSTM,Dense, Dropout, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import pickle as pkl\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from string import digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Q9o_LOdqyOC",
    "outputId": "d22209e8-bfc2-4615-833f-f48b1c3272ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  archive.zip\n",
      "replace Hindi_English_Truncated_Corpus.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
     ]
    }
   ],
   "source": [
    "# !unzip archive.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2CvoskNfrKi5"
   },
   "source": [
    "## **Encoder-Decoder NMT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 536
    },
    "id": "Z8muOiJLq5qA",
    "outputId": "3dba18ce-eca8-49d4-bbbe-4bdd3effeab0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english_sentence</th>\n",
       "      <th>hindi_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>politicians do not have permission to do what ...</td>\n",
       "      <td>राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'd like to tell you about one such child,</td>\n",
       "      <td>मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This percentage is even greater than the perce...</td>\n",
       "      <td>यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what we really mean is that they're bad at not...</td>\n",
       "      <td>हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.The ending portion of these Vedas is called U...</td>\n",
       "      <td>इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127602</th>\n",
       "      <td>Examples of art deco construction can be found...</td>\n",
       "      <td>आर्ट डेको शैली के निर्माण मैरीन ड्राइव और ओवल ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127603</th>\n",
       "      <td>and put it in our cheeks.</td>\n",
       "      <td>और अपने गालों में डाल लेते हैं।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127604</th>\n",
       "      <td>As for the other derivatives of sulphur , the ...</td>\n",
       "      <td>जहां तक गंधक के अन्य उत्पादों का प्रश्न है , द...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127605</th>\n",
       "      <td>its complicated functioning is defined thus in...</td>\n",
       "      <td>Zरचना-प्रकिया को उसने एक पहेली में यों बांधा है .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127606</th>\n",
       "      <td>They've just won four government contracts to ...</td>\n",
       "      <td>हाल ही में उन्हें सरकारी ठेका मिला है करीब सौ ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>127605 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         english_sentence                                     hindi_sentence\n",
       "0       politicians do not have permission to do what ...  राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...\n",
       "1              I'd like to tell you about one such child,  मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...\n",
       "2       This percentage is even greater than the perce...   यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।\n",
       "3       what we really mean is that they're bad at not...     हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते\n",
       "4       .The ending portion of these Vedas is called U...        इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।\n",
       "...                                                   ...                                                ...\n",
       "127602  Examples of art deco construction can be found...  आर्ट डेको शैली के निर्माण मैरीन ड्राइव और ओवल ...\n",
       "127603                          and put it in our cheeks.                    और अपने गालों में डाल लेते हैं।\n",
       "127604  As for the other derivatives of sulphur , the ...  जहां तक गंधक के अन्य उत्पादों का प्रश्न है , द...\n",
       "127605  its complicated functioning is defined thus in...  Zरचना-प्रकिया को उसने एक पहेली में यों बांधा है .\n",
       "127606  They've just won four government contracts to ...  हाल ही में उन्हें सरकारी ठेका मिला है करीब सौ ...\n",
       "\n",
       "[127605 rows x 2 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = pd.read_csv(\"Hindi_English_Truncated_Corpus.csv\")\n",
    "corpus.drop(columns = ['source'], inplace=True)\n",
    "corpus.dropna(inplace=True)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UqZyPzfYq8dU",
    "outputId": "449bb19e-97cd-4311-d8dc-53ad2fe33400"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 2)"
      ]
     },
     "execution_count": 100,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here, we are randomly picking 30k rows from the dataset for the sake of training on our system\n",
    "corpus = corpus.sample(n=15000,random_state=42)\n",
    "corpus.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4WSTeJ5zrWq3"
   },
   "source": [
    "### Preprocessing the data for encoder-decoder input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 262
    },
    "id": "gLfDFvNZrBNn",
    "outputId": "16fec2e4-3450-4611-d177-172d4cc1caca"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english_sentence</th>\n",
       "      <th>hindi_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3556</th>\n",
       "      <td>he declares the result and reports it to the e...</td>\n",
       "      <td>वही परिणाम की घोषणा करता है और निर्वाचन आयोग क...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25899</th>\n",
       "      <td>was a little uncomfortable for them.</td>\n",
       "      <td>थोडा कठिन था।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90924</th>\n",
       "      <td>a multi-purpose auditorium , a branch of the s...</td>\n",
       "      <td>बहुउद्देशीय सभागार , भारतीय स्टेट बैंक की शाखा...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78213</th>\n",
       "      <td>no fees is to be paid for filing the appeal to...</td>\n",
       "      <td>अधिकरण में अपील करने के लिए कोई फीस नहीं देनी ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96955</th>\n",
       "      <td>headind kaun banega crorepati</td>\n",
       "      <td>शीर्षक कौन बनेगा करोड़पति (kaun banega crorepa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        english_sentence                                     hindi_sentence\n",
       "3556   he declares the result and reports it to the e...  वही परिणाम की घोषणा करता है और निर्वाचन आयोग क...\n",
       "25899               was a little uncomfortable for them.                                      थोडा कठिन था।\n",
       "90924  a multi-purpose auditorium , a branch of the s...  बहुउद्देशीय सभागार , भारतीय स्टेट बैंक की शाखा...\n",
       "78213  no fees is to be paid for filing the appeal to...  अधिकरण में अपील करने के लिए कोई फीस नहीं देनी ...\n",
       "96955                      headind kaun banega crorepati  शीर्षक कौन बनेगा करोड़पति (kaun banega crorepa..."
      ]
     },
     "execution_count": 101,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lowercasing all the english/hindi(if any) characters\n",
    "corpus['english_sentence'] = corpus['english_sentence'].apply(lambda x: str(x).lower())\n",
    "corpus['hindi_sentence'] = corpus['hindi_sentence'].apply(lambda x: str(x).lower())\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "BP80qPm-rejm"
   },
   "outputs": [],
   "source": [
    "# Removing quotes from sentences\n",
    "corpus['english_sentence'] = corpus['english_sentence'].apply(lambda x: re.sub(\"'\",'',x))\n",
    "corpus['hindi_sentence'] = corpus['hindi_sentence'].apply(lambda x: re.sub(\"'\",'',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "HLDQXQRtrf5F"
   },
   "outputs": [],
   "source": [
    "spc = set(string.punctuation) # set of all special characters\n",
    "# now, we'll remove the special characters from all the sentences\n",
    "\n",
    "corpus['english_sentence'] = corpus['english_sentence'].apply(lambda x: ''.join(ch for ch in x if ch not in spc))\n",
    "corpus['hindi_sentence'] = corpus['hindi_sentence'].apply(lambda x: ''.join(ch for ch in x if ch not in spc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "i3C66NEvrmVZ"
   },
   "outputs": [],
   "source": [
    "# Removing all the numbers from text\n",
    "remove_digits = str.maketrans('', '', digits)\n",
    "corpus['english_sentence'] = corpus['english_sentence'].apply(lambda x: x.translate(remove_digits))\n",
    "corpus['hindi_sentence'] = corpus['hindi_sentence'].apply(lambda x: x.translate(remove_digits))\n",
    "\n",
    "corpus['english_sentence'] = corpus['english_sentence'].apply(lambda x: re.sub(\"[0123456789]\",\"\",x))\n",
    "corpus['hindi_sentence'] = corpus['hindi_sentence'].apply(lambda x: re.sub(\"[0123456789]\",\"\",x))\n",
    "corpus['hindi_sentence'] = corpus['hindi_sentence'].apply(lambda x: re.sub(\"[०१२३४५६७८९१०]\",\"\",x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "Pbm9sd_1roul"
   },
   "outputs": [],
   "source": [
    "# removing the extra spaces from both hindi and english corpus\n",
    "corpus['english_sentence'] = corpus['english_sentence'].apply(lambda x: x.strip())\n",
    "corpus['hindi_sentence'] = corpus['hindi_sentence'].apply(lambda x: x.strip())\n",
    "\n",
    "corpus['english_sentence'] = corpus['english_sentence'].apply(lambda x: re.sub(' +',' ',x))\n",
    "corpus['hindi_sentence'] = corpus['hindi_sentence'].apply(lambda x: re.sub(' +',' ',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "_UmIn_7urz9W"
   },
   "outputs": [],
   "source": [
    "# Adding the start and end tokems to target sequence so that the decoder knows when to start and stop\n",
    "corpus['hindi_sentence'] = corpus['hindi_sentence'].apply(lambda x: 'START_ '+ x + ' _END')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 262
    },
    "id": "AdYg40IRr1vf",
    "outputId": "60d8960a-b681-4355-ea67-776e2fac3068"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english_sentence</th>\n",
       "      <th>hindi_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3556</th>\n",
       "      <td>he declares the result and reports it to the e...</td>\n",
       "      <td>START_ वही परिणाम की घोषणा करता है और निर्वाचन...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25899</th>\n",
       "      <td>was a little uncomfortable for them</td>\n",
       "      <td>START_ थोडा कठिन था। _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90924</th>\n",
       "      <td>a multipurpose auditorium a branch of the stat...</td>\n",
       "      <td>START_ बहुउद्देशीय सभागार भारतीय स्टेट बैंक की...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78213</th>\n",
       "      <td>no fees is to be paid for filing the appeal to...</td>\n",
       "      <td>START_ अधिकरण में अपील करने के लिए कोई फीस नही...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96955</th>\n",
       "      <td>headind kaun banega crorepati</td>\n",
       "      <td>START_ शीर्षक कौन बनेगा करोड़पति kaun banega c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        english_sentence                                     hindi_sentence\n",
       "3556   he declares the result and reports it to the e...  START_ वही परिणाम की घोषणा करता है और निर्वाचन...\n",
       "25899                was a little uncomfortable for them                          START_ थोडा कठिन था। _END\n",
       "90924  a multipurpose auditorium a branch of the stat...  START_ बहुउद्देशीय सभागार भारतीय स्टेट बैंक की...\n",
       "78213  no fees is to be paid for filing the appeal to...  START_ अधिकरण में अपील करने के लिए कोई फीस नही...\n",
       "96955                      headind kaun banega crorepati  START_ शीर्षक कौन बनेगा करोड़पति kaun banega c..."
      ]
     },
     "execution_count": 107,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GGkFk2l3r7NA"
   },
   "source": [
    "## Creating the Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8wVKltGHr4nR",
    "outputId": "1f872f5b-3c30-4545-841c-122c20457fe3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in English Vocabulary:  22998\n",
      "Words in Hindi Vocabulary:  27767\n"
     ]
    }
   ],
   "source": [
    "# creating hindi and english vocabulary\n",
    "eng_words = set()\n",
    "for eng in corpus['english_sentence']:\n",
    "    for word in eng.split():\n",
    "        if word not in eng_words:\n",
    "            eng_words.add(word)\n",
    "            \n",
    "hin_words = set()\n",
    "for hin in corpus['hindi_sentence']:\n",
    "    for word in hin.split():\n",
    "        if word not in hin_words:\n",
    "            hin_words.add(word)\n",
    "            \n",
    "print('Words in English Vocabulary: ',len(eng_words))\n",
    "print('Words in Hindi Vocabulary: ', len(hin_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 450
    },
    "id": "DWMy-FJcsABM",
    "outputId": "67c6e3e4-3be6-4570-d70a-13e7fbdac00b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english_sentence</th>\n",
       "      <th>hindi_sentence</th>\n",
       "      <th>len_english_sentences</th>\n",
       "      <th>len_hindi_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3556</th>\n",
       "      <td>he declares the result and reports it to the e...</td>\n",
       "      <td>START_ वही परिणाम की घोषणा करता है और निर्वाचन...</td>\n",
       "      <td>19</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25899</th>\n",
       "      <td>was a little uncomfortable for them</td>\n",
       "      <td>START_ थोडा कठिन था। _END</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90924</th>\n",
       "      <td>a multipurpose auditorium a branch of the stat...</td>\n",
       "      <td>START_ बहुउद्देशीय सभागार भारतीय स्टेट बैंक की...</td>\n",
       "      <td>32</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78213</th>\n",
       "      <td>no fees is to be paid for filing the appeal to...</td>\n",
       "      <td>START_ अधिकरण में अपील करने के लिए कोई फीस नही...</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96955</th>\n",
       "      <td>headind kaun banega crorepati</td>\n",
       "      <td>START_ शीर्षक कौन बनेगा करोड़पति kaun banega c...</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        english_sentence  ... len_hindi_sentences\n",
       "3556   he declares the result and reports it to the e...  ...                  22\n",
       "25899                was a little uncomfortable for them  ...                   5\n",
       "90924  a multipurpose auditorium a branch of the stat...  ...                  31\n",
       "78213  no fees is to be paid for filing the appeal to...  ...                  13\n",
       "96955                      headind kaun banega crorepati  ...                   9\n",
       "\n",
       "[5 rows x 4 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the max_length for source and target sentences\n",
    "corpus['len_english_sentences'] = corpus['english_sentence'].apply(lambda x: len(x.split(\" \")))\n",
    "corpus['len_hindi_sentences'] = corpus['hindi_sentence'].apply(lambda x: len(x.split(\" \")))\n",
    "\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q7bBWneysBsQ",
    "outputId": "49cd3412-af0a-4b8a-d3f9-63a09e42f7b6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12709, 4)"
      ]
     },
     "execution_count": 110,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = corpus[corpus.len_english_sentences<=30]\n",
    "corpus = corpus[corpus.len_hindi_sentences<=30]\n",
    "corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FR7ftZnjsDbG",
    "outputId": "148ba498-b861-435d-8246-849d45f787bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English:  30\n",
      "Hindi:  30\n"
     ]
    }
   ],
   "source": [
    "max_length_source = max(corpus['len_english_sentences'])\n",
    "max_length_target = max(corpus['len_hindi_sentences'])\n",
    "\n",
    "print(\"English: \",max_length_source)\n",
    "print(\"Hindi: \",max_length_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "VaHO3d03sNwS"
   },
   "outputs": [],
   "source": [
    "# Now we'll convert the given word into an integer index and vice versa\n",
    "\n",
    "source_words = sorted(list(eng_words))\n",
    "target_words = sorted(list(hin_words))\n",
    "num_encoder_tokens = len(eng_words)\n",
    "num_decoder_tokens = len(hin_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "LF2UmqWgsYYm"
   },
   "outputs": [],
   "source": [
    "# this has been done for zero padding\n",
    "num_encoder_tokens+=1\n",
    "num_decoder_tokens+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "m2UEtLeQsbOb"
   },
   "outputs": [],
   "source": [
    "# Assigning an index to each token present in the vocabulary\n",
    "source_token_index = dict([(word,i+1) for i,word in enumerate(source_words)])\n",
    "target_token_index = dict([(word,i+1) for i,word in enumerate(target_words)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "xDBxzPiOsc1S"
   },
   "outputs": [],
   "source": [
    "# reversing the word index format to index word format to generate outputs\n",
    "reverse_source_char_index = dict((i,word) for word,i in source_token_index.items())\n",
    "reverse_target_char_index = dict((i,word) for word,i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k570pfoysiA2"
   },
   "source": [
    "## Preparing the data for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fKTRfsosmOW"
   },
   "source": [
    "In order to proceed with this session I want you to run and understand the piece of code that is written here:\n",
    "```python\n",
    "txt = 'we are learning encoder-decoder architecture'\n",
    "for t,word in enumerate(txt.split()):\n",
    "    print(t,\" \",word)\n",
    "print(len(txt.split())-1)\n",
    "source_token_index['my']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "lep0wHZIsevG"
   },
   "outputs": [],
   "source": [
    "source, target = corpus['english_sentence'], corpus['hindi_sentence']\n",
    "X_train, X_test, y_train, y_test = train_test_split(source,target,test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "XskXzfZbsr4e"
   },
   "outputs": [],
   "source": [
    "# this particular block here is taking the batch and is performing one-hot-encoding\n",
    "# this function will generate the data in batches for testing and training\n",
    "def generate_batch(X = X_train, y = y_train, batch_size = 128):\n",
    "    while True:\n",
    "        for j in range(0,len(X),batch_size):\n",
    "            encoder_input_data = np.zeros((batch_size, max_length_source),dtype='float32')\n",
    "            decoder_input_data = np.zeros((batch_size, max_length_target),dtype='float32')\n",
    "            decoder_target_data = np.zeros((batch_size,max_length_target,num_decoder_tokens),dtype='float32')\n",
    "            for i, (input_text,target_text) in enumerate(zip(X[j:j+batch_size],y[j:j+batch_size])):\n",
    "                for t,word in enumerate(input_text.split()):\n",
    "                    encoder_input_data[i,t] = source_token_index[word] # encoder input sequence\n",
    "                for t,word in enumerate(target_text.split()):\n",
    "                    if t<len(target_text.split())-1:\n",
    "                        # this means that the sentence is not completed hence it will be put back as the decoder input\n",
    "                        decoder_input_data[i,t] = target_token_index[word] # decoder input sequence\n",
    "                    if t>0:\n",
    "                        # this will run only after the first index or the first decoder output hence t>0\n",
    "                        # decoder target sequence (one hot encoded)\n",
    "                        # does not include StarSeq token\n",
    "                        # offset by one timestep\n",
    "                        # and here we are feeding the output of the previous time stamp\n",
    "                        decoder_target_data[i,t-1,target_token_index[word]] = 1.\n",
    "            yield([encoder_input_data,decoder_input_data],decoder_target_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OhkyAX7zs0cx"
   },
   "source": [
    "## Encoder-Decoder Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jrk4nU7ss6eG"
   },
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "iy8yXdzHs5pV"
   },
   "outputs": [],
   "source": [
    "latent_dim = 64\n",
    "\n",
    "# this None below indicates that the sequences can have arbitrary length\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "encoder_embeddings = Embedding(num_encoder_tokens,latent_dim,mask_zero=True)(encoder_inputs)\n",
    "# mask zero is basically used for padding the sequences with zero so that all are of similar length\n",
    "\n",
    "encoder_lstm = LSTM(latent_dim,return_state=True)\n",
    "encoder_outputs, state_h,state_c = encoder_lstm(encoder_embeddings)\n",
    "# we discard the encoder_outputs and only keep the states because that's what we need here for the encoder part\n",
    "\n",
    "encoder_states = [state_h,state_c] # output states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XFGAr2rvs9p2"
   },
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "MXx488ees_EN"
   },
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None,))\n",
    "decoder_embedding_layer = Embedding(num_decoder_tokens,latent_dim,mask_zero=True)\n",
    "decoder_embeddings = decoder_embedding_layer(decoder_inputs)\n",
    "\n",
    "# decoder must return the full output sequence, and internal states.\n",
    "# we don't use the return states in training model but we'll use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim,return_sequences=True,return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embeddings, initial_state=encoder_states)\n",
    "\n",
    "# here we'll also add a dense layer to get the outputs\n",
    "decoder_dense = Dense(num_decoder_tokens,activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs) # here the output is fed back into the decoder layer\n",
    "\n",
    "# hence we'll define a model that will turn the encoder_input_data and decoder_input_data into decoder_target_data\n",
    "model = Model([encoder_inputs,decoder_inputs],decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ws0KDixtBhH",
    "outputId": "0d517ac7-0110-4ebf-e79d-3374227cdd77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_19\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_13 (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_14 (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, None, 64)     1471936     input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, None, 64)     1777152     input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   [(None, 64), (None,  33024       embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                   [(None, None, 64), ( 33024       embedding_5[0][0]                \n",
      "                                                                 lstm_4[0][1]                     \n",
      "                                                                 lstm_4[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, None, 27768)  1804920     lstm_5[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 5,120,056\n",
      "Trainable params: 5,120,056\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# now that our model has been created we'll train it using 100 epochs\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "PYfiTkD4tEuV"
   },
   "outputs": [],
   "source": [
    "train_sent = len(X_train)\n",
    "val_sent = len(X_test)\n",
    "batch_size = 128\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sp696gjmtJA1",
    "outputId": "63d9411f-503a-47e5-e7af-1dfd260a7f69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "89/89 [==============================] - 415s 5s/step - loss: 3.7421 - val_loss: 3.1337\n",
      "Epoch 2/30\n",
      "89/89 [==============================] - 409s 5s/step - loss: 3.0994 - val_loss: 3.1279\n",
      "Epoch 3/30\n",
      "89/89 [==============================] - 409s 5s/step - loss: 3.0590 - val_loss: 3.1003\n",
      "Epoch 4/30\n",
      "89/89 [==============================] - 407s 5s/step - loss: 2.9957 - val_loss: 3.0514\n",
      "Epoch 5/30\n",
      "89/89 [==============================] - 407s 5s/step - loss: 2.9438 - val_loss: 3.0288\n",
      "Epoch 6/30\n",
      "89/89 [==============================] - 407s 5s/step - loss: 2.9055 - val_loss: 3.0135\n",
      "Epoch 7/30\n",
      "89/89 [==============================] - 407s 5s/step - loss: 2.8732 - val_loss: 3.0034\n",
      "Epoch 8/30\n",
      "89/89 [==============================] - 407s 5s/step - loss: 2.8478 - val_loss: 2.9973\n",
      "Epoch 9/30\n",
      "89/89 [==============================] - 413s 5s/step - loss: 2.8249 - val_loss: 2.9908\n",
      "Epoch 10/30\n",
      "89/89 [==============================] - 407s 5s/step - loss: 2.8033 - val_loss: 2.9801\n",
      "Epoch 11/30\n",
      "89/89 [==============================] - 406s 5s/step - loss: 2.7795 - val_loss: 2.9687\n",
      "Epoch 12/30\n",
      "89/89 [==============================] - 403s 5s/step - loss: 2.7510 - val_loss: 2.9526\n",
      "Epoch 13/30\n",
      "89/89 [==============================] - 400s 4s/step - loss: 2.7197 - val_loss: 2.9345\n",
      "Epoch 14/30\n",
      "89/89 [==============================] - 400s 4s/step - loss: 2.6883 - val_loss: 2.9176\n",
      "Epoch 15/30\n",
      "89/89 [==============================] - 402s 5s/step - loss: 2.6602 - val_loss: 2.9055\n",
      "Epoch 16/30\n",
      "89/89 [==============================] - 405s 5s/step - loss: 2.6328 - val_loss: 2.9036\n",
      "Epoch 17/30\n",
      "89/89 [==============================] - 403s 5s/step - loss: 2.6058 - val_loss: 2.8981\n",
      "Epoch 18/30\n",
      "89/89 [==============================] - 404s 5s/step - loss: 2.5829 - val_loss: 2.8802\n",
      "Epoch 19/30\n",
      "89/89 [==============================] - 401s 5s/step - loss: 2.5614 - val_loss: 2.8692\n",
      "Epoch 20/30\n",
      "89/89 [==============================] - 403s 5s/step - loss: 2.5350 - val_loss: 2.8631\n",
      "Epoch 21/30\n",
      "89/89 [==============================] - 399s 4s/step - loss: 2.5083 - val_loss: 2.8540\n",
      "Epoch 22/30\n",
      "89/89 [==============================] - 400s 4s/step - loss: 2.4826 - val_loss: 2.8486\n",
      "Epoch 23/30\n",
      "89/89 [==============================] - 400s 4s/step - loss: 2.4562 - val_loss: 2.8454\n",
      "Epoch 24/30\n",
      "89/89 [==============================] - 400s 4s/step - loss: 2.4342 - val_loss: 2.8390\n",
      "Epoch 25/30\n",
      "89/89 [==============================] - 399s 4s/step - loss: 2.4120 - val_loss: 2.8324\n",
      "Epoch 26/30\n",
      "89/89 [==============================] - 399s 4s/step - loss: 2.3903 - val_loss: 2.8334\n",
      "Epoch 27/30\n",
      "89/89 [==============================] - 400s 4s/step - loss: 2.3673 - val_loss: 2.8278\n",
      "Epoch 28/30\n",
      "89/89 [==============================] - 399s 4s/step - loss: 2.3480 - val_loss: 2.8229\n",
      "Epoch 29/30\n",
      "89/89 [==============================] - 400s 4s/step - loss: 2.3280 - val_loss: 2.8231\n",
      "Epoch 30/30\n",
      "89/89 [==============================] - 401s 5s/step - loss: 2.3046 - val_loss: 2.8243\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff3383adf98>"
      ]
     },
     "execution_count": 122,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training the model, this model is giving us the resource exhausted error, hence, we'll take it to google colab for now\n",
    "model.fit_generator(generator = generate_batch(X_train, y_train, batch_size=batch_size),\n",
    "                   steps_per_epoch=train_sent//batch_size,\n",
    "                   epochs=epochs,\n",
    "                   validation_data=generate_batch(X_test,y_test,batch_size=batch_size),\n",
    "                   validation_steps=val_sent//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "m538ybHYvE5U"
   },
   "outputs": [],
   "source": [
    "model.save_weights('weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "id": "GbdZSyxRJIdd"
   },
   "outputs": [],
   "source": [
    "## now we'll encode the input sequence to get the \"thought vectors\"\n",
    "encoder_model = Model(encoder_inputs,encoder_states)\n",
    "\n",
    "# Decoder setup, below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h,decoder_state_input_c]\n",
    "\n",
    "# get the embeddings of the decoder sequence\n",
    "decoder_embeddings_2 = decoder_embedding_layer(decoder_inputs)\n",
    "\n",
    "# To preduct the next word in the sequence, set the initial states to the states from previous time step\n",
    "decoder_outputs_2,state_h_2,state_c_2 = decoder_lstm(decoder_embeddings_2, initial_state=decoder_states_inputs)\n",
    "decoder_states_2 = [state_h_2,state_c_2]\n",
    "decoder_outputs_2 = decoder_dense(decoder_outputs_2)\n",
    "# here, in the above code we've used the dense softmax layer to generate probability distance over the target vocabulary\n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model([decoder_inputs]+decoder_states_inputs,\n",
    "                      [decoder_outputs_2]+ decoder_states_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "id": "UEnNgL51LcPm"
   },
   "outputs": [],
   "source": [
    "def decoder_sequence(input_seq):\n",
    "  # encode the input as state vectors,\n",
    "  states_value = encoder_model.predict(input_seq)\n",
    "  # generate empty target sequence of length 1\n",
    "  target_seq = np.zeros((1,1))\n",
    "\n",
    "  # add the start character as the first character of the target sequence\n",
    "  target_seq[0,0] = target_token_index['START_']\n",
    "\n",
    "  # sampling loop for a batch of sequences\n",
    "  stop_condition = False\n",
    "  decoded_sentence = ''\n",
    "  while not stop_condition:\n",
    "    output_tokens, h, c = decoder_model.predict([target_seq]+states_value)\n",
    "\n",
    "    # sample a token\n",
    "    sampled_token_index = np.argmax(output_tokens[0,-1,:])\n",
    "    sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "    # here we've generated the first word of the sequence using the decoder\n",
    "    decoded_sentence += ' '+sampled_char\n",
    "\n",
    "    # exit codition: either hit max_length or find the stop character\n",
    "    if (sampled_char == '_END' or len(decoded_sentence)>50):\n",
    "      stop_condition = True\n",
    "\n",
    "    # update the target sequence for the next time step\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0,0] = sampled_token_index # for the next time step\n",
    "\n",
    "    # update the states \n",
    "    states_value = [h,c]\n",
    "\n",
    "  return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "id": "F_76_nhYNl-s"
   },
   "outputs": [],
   "source": [
    "train_gen = generate_batch(X_train, y_train, batch_size = 1)\n",
    "k=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YHUrHp6PN4QD",
    "outputId": "b695874b-18a1-4f1a-81b1-a3759a5b940b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[22218. 18371.  9322. 10931.  2006.  4981.     0.     0.     0.     0.\n",
      "      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
      "Input English sentence: what should i know before deciding\n",
      "Actual Hindi Translation:  फैसला लेने से पहले मुझे क्या जानना जरुरी है \n",
      "Predicted Hindi Translation:  क्योंकि आप के बारे में क्या \n"
     ]
    }
   ],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "print(input_seq)\n",
    "decoded_sent = decoder_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Hindi Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Hindi Translation:', decoded_sent[:-4])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Machine_Translation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
